<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Kubernetes on 박성훈의 블로그</title>
    <link>https://pseonghoon.github.io/tags/kubernetes/</link>
    <description>Recent content in Kubernetes on 박성훈의 블로그</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>2020</copyright>
    <lastBuildDate>Fri, 05 Aug 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://pseonghoon.github.io/tags/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Kubernetes] 애플리케이션 트러블슈팅</title>
      <link>https://pseonghoon.github.io/post/k8s-application-troubleshooting/</link>
      <pubDate>Fri, 05 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>https://pseonghoon.github.io/post/k8s-application-troubleshooting/</guid>
      <description>
        
          &lt;p&gt;쿠버네티스(Kubernetes) 에서 컨테이너가 정상적으로 시작하지 못하거나 애플리케이션이 제대로 동작하지 않을 때 트러블슈팅할 수 있는 방법을 공유한다.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; 이 포스트에 포함된 kubectl 명령을 직접 실행하는 것보다 &lt;a href=&#34;https://github.com/derailed/k9s&#34;&gt;k9s&lt;/a&gt;를 사용하는 것이 편리하다.&lt;/p&gt;
&lt;h1 id=&#34;kubectl로-이벤트-확인&#34;&gt;kubectl로 이벤트 확인&lt;/h1&gt;
&lt;p&gt;이벤트가 문제의 원인에 대한 중요한 정보를 제공할 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;kubectl describe pod&lt;/code&gt; 명령으로 파드(Pod)와 연관된 최근 이벤트를 확인할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl describe pod &amp;lt;pod-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;아래와 같이 결과의 마지막에 파드 관련 이벤트들이 표시된다. 아래 예제에선 startup probe가 실패한 것이 원인이다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;Name:         psh-test-5bd7d665c8-24vfn
&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;Namespace:    psh
&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;Priority:     0
&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;Node:         ip-10-73-3-41.ap-northeast-2.compute.internal/10.73.3.41
&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;Start Time:   Mon, 08 Aug 2022 23:53:44 +0900
&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;... 중간 생략 ... 
&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;Events:
&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;  Type     Reason     Age   From               Message
&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;  ----     ------     ----  ----               -------
&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;  Normal   Scheduled  16s   default-scheduler  Successfully assigned psh/psh-test-5bd7d665c8-24vfn to ip-10-73-3-41.ap-northeast-2.compute.internal
&lt;span class=&#34;ln&#34;&gt;13&lt;/span&gt;  Normal   Pulled     15s   kubelet            Container image &amp;#34;123456789012.dkr.ecr.ap-northeast-2.amazonaws.com/helloworld/web:latest&amp;#34; already present on machine
&lt;span class=&#34;ln&#34;&gt;14&lt;/span&gt;  Normal   Created    15s   kubelet            Created container psh-test
&lt;span class=&#34;ln&#34;&gt;15&lt;/span&gt;  Normal   Started    15s   kubelet            Started container psh-test
&lt;span class=&#34;ln&#34;&gt;16&lt;/span&gt;  Warning  Unhealthy  10s   kubelet            Startup probe failed: Get &amp;#34;http://10.73.12.174:8080/not_exist&amp;#34;: dial tcp 10.73.12.174:8080: connect: connection refused
&lt;span class=&#34;ln&#34;&gt;17&lt;/span&gt;  Warning  Unhealthy  6s    kubelet            Startup probe failed: HTTP probe failed with statuscode: 404
&lt;span class=&#34;ln&#34;&gt;18&lt;/span&gt;  Normal   Killing    6s    kubelet            Container psh-test failed startup probe, will be restarted
&lt;span class=&#34;ln&#34;&gt;19&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;kubectl get event&lt;/code&gt; 명령으로 클러스터에서 최근 발생한 이벤트들을 확인할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl get events -n &amp;lt;namespace&amp;gt;
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;$ kubectl get events --all-namespaces 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;event-exporter로-오래된-이벤트-확인&#34;&gt;event-exporter로 오래된 이벤트 확인&lt;/h1&gt;
&lt;p&gt;이벤트는 etcd의 저장 공간 제한 때문에 보통 몇 시간 정도 밖에 보관되지 않는다.
따라서 문제가 발생 후 시간이 지나면 이벤트를 통해  원인을 파악하기 어렵게 된다.&lt;/p&gt;
&lt;p&gt;이 문제에 대응하기 위해서 클러스터에 &lt;a href=&#34;https://github.com/opsgenie/kubernetes-event-exporter&#34;&gt;kubernetes-event-exporter&lt;/a&gt;를 설치하는 방법이 있다.
이벤트가 생성될 때 event-exporter가 그 내용을 로그로 남긴다. 이 로그를 AWS CloudWatch Logs 같은 로그 저장소에 전송하고 필요할 때 검색하면 된다. AWS CloudWatch Logs 를 사용한다면 CloudWatch Logs Insights 기능으로 검색하면 편리하다.&lt;/p&gt;
&lt;p&gt;조회할 로그 그룹을 /aws/containerinsights/&lt;code&gt;&amp;lt;cluster-name&amp;gt;&lt;/code&gt;/application 으로 설정한다.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://pseonghoon.github.io/post/img/log_group.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;아래 예제처럼 쿼리를 실행한다.&lt;/p&gt;
&lt;!--
```
fields @timestamp, parsed.involvedObject.name, parsed.message
| filter kubernetes.labels.app = &#39;event-exporter&#39; and parsed.involvedObject.name like &#39;node-exporter-qz87v&#39;
| sort @timestamp asc
```
--&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;fields @timestamp, @message
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;| filter kubernetes.container_name = &amp;#39;event-exporter&amp;#39;
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;| sort @timestamp desc
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;| limit 50
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;em&gt;Note:&lt;/em&gt; 실제로 이벤트가 발생한 횟수보다 kubernetes-event-exporter 에 남는 로그 수가 더 적다. 쿠버네티스는 동일한 이벤트가 근접한 시간에 여러번 발생하면 기존에 etcd에 저장된 이벤트를 업데이트하는데 아마도 kubernetes-event-exporter가 이벤트가 새로 etcd에 생성되는 경우에만 로그를 남기기 때문인 것 같다.&lt;/p&gt;
&lt;h1 id=&#34;kubectl-logs-명령으로-로그-확인&#34;&gt;kubectl logs 명령으로 로그 확인&lt;/h1&gt;
&lt;p&gt;컨테이너가 표준 출력(standard out)으로 로그를 출력한다면 kubectl logs 명령으로 application의 로그를 볼 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl logs &amp;lt;pod-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;사용 방법: &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs&#34;&gt;https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#logs&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Pod이 CrashLoopBackOff 상태일 때는 -p 옵션 추가&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;kubectl logs 명령은 현재 실행 중인 컨테이너의 로그를 출력하기 때문에 컨테이너가 에러를 내고 종료해버리면 에러 로그를 확인할 수 없다. 이럴 때는 &lt;mark&gt; -p ( --previous=true ) 옵션을 추가하면 종료된 컨테이너의 로그를 볼 수 있다.&lt;/mark&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl logs -p &amp;lt;pod-name&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;로그-저장소에서-로그-조회&#34;&gt;로그 저장소에서 로그 조회&lt;/h1&gt;
&lt;p&gt;application 로그는 CloudWatch Logs, Elasticsearch, Loki 등의 로그 저장소에 전송해야 한다. 문제가 발생했을 때 해당하는 로그를 조회해서 확인할 수 있다.&lt;/p&gt;
&lt;p&gt;예를 들어 &lt;a href=&#34;https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/Container-Insights-EKS-logs.html&#34;&gt;CloudWatch Logs에 컨테이너 로그를 전송&lt;/a&gt;하고 있다면 CloudWatch Logs Insights의 쿼리를 사용해서 로그를 조회할 수 있다.&lt;/p&gt;
&lt;p&gt;먼저 로그 그룹을 /aws/containerinsights/&lt;code&gt;&amp;lt;cluster-name&amp;gt;&lt;/code&gt;/application 으로 지정한다.&lt;/p&gt;
&lt;p&gt;Pod 이름을 이용하여 아래처럼 조회할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;fields @timestamp, log,  @message
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;| filter kubernetes.pod_name like &amp;#34;cluster-autoscaler&amp;#34;   
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;| sort @timestamp desc
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;| limit 20
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;컨테이터의-last-state-확인&#34;&gt;컨테이터의 Last State 확인&lt;/h1&gt;
&lt;p&gt;컨테이너가 application 내부적인 이유로 에러를 내고 종료,재시작을 반복하면 Pod과 컨테이너가 CrashLoopBackOff 상태가 된다.&lt;/p&gt;
&lt;p&gt;kubectl logs -p 명령에서도 유용한 내용이 없다면 kubectl describe Pod 명령으로 Last State의 Reason, Exit Code를 확인한다.&lt;/p&gt;
&lt;p&gt;OOM으로 죽은 경우에는 Reason에 그렇게 표시가 된다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;nginx&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Container ID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;   &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;docker://513cab3de8be8754d054a4eff45e291d33b63e11b2143d0ff782dccc286ba05e&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;nginx&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Image ID&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;docker-pullable://nginx@sha256:c4ee0ecb376636258447e1d8effb56c09c75fe7acf756bf7c13efadf38aa0aca&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Port&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;           &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;&amp;lt;none&amp;gt;&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Waiting&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Reason&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;CrashLoopBackOff&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Last State&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Terminated&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Reason&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;       &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Error&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Exit Code&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Started&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Tue, 27 Mar 2018 19:13:15 +0200&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;Finished&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;     &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Tue, 27 Mar 2018 19:13:16 +0200&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;지금까지 기술한 방법으로 원인을 알 수 없다면 각 application에 맞는 디버거를 사용해야 한다.&lt;/p&gt;
&lt;h1 id=&#34;kubectl-exec로-컨테이너-내부에서-쉘-실행하기&#34;&gt;kubectl exec로 컨테이너 내부에서 쉘 실행하기&lt;/h1&gt;
&lt;p&gt;아래 문서를 참고한다.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/&#34;&gt;https://kubernetes.io/docs/tasks/debug-application-cluster/get-shell-running-container/&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;kubectl-port-foward-명령으로-acl에-제약-받지-않고-연결-테스트&#34;&gt;kubectl port-foward 명령으로 ACL에 제약 받지 않고 연결 테스트&lt;/h1&gt;
&lt;p&gt;Service, Pod의 port 로 포워딩되는 로컬 포트를 열수 있다.&lt;/p&gt;
&lt;p&gt;kubectl 명령만 실행할 수 있으면 클러스터 내부 통신만 하는 앱이거나 ACL에서 허용하지 않는 경우라도 로컬 컴퓨터에서 접속할 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/&#34;&gt;https://kubernetes.io/docs/tasks/access-application-cluster/port-forward-access-application-cluster/&lt;/a&gt;&lt;/p&gt;
        
      </description>
    </item>
    
    <item>
      <title>[Kubernetes] Cluster Autocaler의 scale-down 튜닝</title>
      <link>https://pseonghoon.github.io/post/ca-scale-down-optimization/</link>
      <pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pseonghoon.github.io/post/ca-scale-down-optimization/</guid>
      <description>
        
          &lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler&#34;&gt;Cluster Autoscaler(CA)&lt;/a&gt;는 클러스터의 노드 개수를 적절하게 유지하는 역할을 한다.
새로 생성된 pod이 실행될 노드가 없어 &lt;code&gt;Pending&lt;/code&gt; 상태라면 노드를 생성하고,
반대로 필요한 리소스에 비해 노드 수가 불필요하게 많다면 한가한 노드를 없애서 리소스 집적도를 높이는 scale-down을 한다.&lt;/p&gt;
&lt;p&gt;CA가 특정 노드를 없애려면 몇가지 제약 조건에 해당하지 않아야 하는데, 이런 제약 조건 때문에 scale-down 효율이 떨어지면 불필요한 비용을 지출하게 될 수 있다.&lt;/p&gt;
&lt;p&gt;CA의 scale-down 효율을 높여서 비용 절감에 효과를 본 방법을 정리했다. 요약하면 아래와 같다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;--skip-nodes-with-local-storage 옵션을 false로 설정&lt;/li&gt;
&lt;li&gt;--scale-down-utilization-threshold 옵션값 올리기&lt;/li&gt;
&lt;li&gt;application의 PodDisruptionBudget(PDB)가 scale-down을 막고 있는 경우 PDB 수정&lt;/li&gt;
&lt;li&gt;kube-system namespace에 속한 pod에 PodDisruptionBudget을 설정 (kube-dns 등)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cluster-autoscaler의-scale-down-조건&#34;&gt;Cluster Autoscaler의 scale-down 조건&lt;/h1&gt;
&lt;p&gt;Clusster Autosacler &lt;a href=&#34;https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node&#34;&gt;FAQ 문서&lt;/a&gt;에 잘 기술되어 있다.&lt;/p&gt;
&lt;p&gt;노드에 아래 조건에 해당하는 pod이 있으면 CA가 노드를 제거할 수 없다. 이 포스트의 맥락에 해당하는 것만 추렸다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;로컬 스토리지를 사용하는 경우&lt;/li&gt;
&lt;li&gt;PodDisruptionBudget에 의해 pod을 종료할 수 없는 경우&lt;/li&gt;
&lt;li&gt;kube-system namespace에 속한 pod. 적절한 PodDisruptionBudget 설정을 해주면 이 제한을 풀 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;현재-노드-개수가-적절한지-확인하는-방법&#34;&gt;현재 노드 개수가 적절한지 확인하는 방법&lt;/h1&gt;
&lt;p&gt;우선 현재 scale-down을 충분히 효율적으로 하고 있는지 확인해야 한다.
노드의 &amp;lsquo;리소스 활용률&amp;rsquo; 이 충분히 높아야 하는데, 아래처럼 계산할 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;리소스 활용률 = (노드에서 실행되는 컨테이너들의 resource request 합)/(노드의 resource capacity)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;컨테이너에서 실행되는 프로세스들이 실제 사용하는 수치가 아니라 컨테이너의 resource 설정에서 cpu나 memory request 값을 따지는 것이다.&lt;/p&gt;
&lt;p&gt;노드별 리스스 활용률은 kubectl로 대략 파악할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl describe nodes | grep Resource --after=3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;kubectl에 &lt;a href=&#34;https://github.com/etopeter/kubectl-view-utilization&#34;&gt;view-utilization plugin&lt;/a&gt;을 설치하면 더 쉽게 확인할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl view-utilization &lt;span class=&#34;c1&#34;&gt;# 클러스터 전체 현황&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;$ kubectl view-utilization nodes &lt;span class=&#34;c1&#34;&gt;# 노드별 현황&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;아래는 kubectl view-utilization 명령의 실행 결과 예시이다.&lt;/p&gt;
&lt;figure&gt;&lt;img src=&#34;https://pseonghoon.github.io/post/img/kubectl_view_utilization.png&#34;/&gt;
&lt;/figure&gt;

&lt;p&gt;제대로 관리하려면 결국 Prometheus 같은 모니터링 시스템의 관리 대상에 포함해야 한다.&lt;/p&gt;
&lt;p&gt;kube-state-metrics가 제공하는 metric을 조회하는 Prometheus 쿼리는 아래와 같다.&lt;/p&gt;
&lt;p&gt;클러스터 CPU 리소스 활용률 평균: &lt;code&gt;sum (kube_pod_container_resource_requests{resource=&amp;quot;cpu&amp;quot;}) / sum (kube_node_status_capacity{resource=&amp;quot;cpu&amp;quot;} )&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;클러스터 메모리 리소스 활용률 평균: &lt;code&gt;sum (kube_pod_container_resource_requests{resource=&amp;quot;memory&amp;quot;}) / sum (kube_node_status_capacity{resource=&amp;quot;memory&amp;quot;} )&lt;/code&gt;&lt;/p&gt;
&lt;h1 id=&#34;ca의-로그로-scale-down을-하지-않는-이유-확인&#34;&gt;CA의 로그로 scale-down을 하지 않는 이유 확인&lt;/h1&gt;
&lt;p&gt;CA는 특정 노드를 scale-down 대상으로 취급하지 않는지 이유를 로그로 자세히 알려준다.
아래 명령으로 확인할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl logs -f  -n kube-system &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get pod -o Name -n kube-system -l app.kubernetes.io/instance&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;cluster-autoscaler&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;로그의 예는 아래와 같다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;I1119 16:13:22.567346       1 cluster.go:148] Fast evaluation: ip-10-76-141-178.ap-northeast-2.compute.internal for removal
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;I1119 16:13:22.567357       1 cluster.go:169] Fast evaluation: node ip-10-76-141-178.ap-northeast-2.compute.internal cannot be removed: pod with local storage present: istiod-1-10-4-6c66f78678-7wl5r
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;...
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;5&lt;/span&gt;I1119 16:18:57.113554       1 scale_down.go:443] Node ip-10-91-71-213.ap-northeast-2.compute.internal is not suitable for removal - cpu utilization too big (0.917098)
&lt;span class=&#34;ln&#34;&gt;6&lt;/span&gt;I1119 16:18:57.113607       1 scale_down.go:443] Node ip-10-91-70-122.ap-northeast-2.compute.internal is not suitable for removal - memory utilization too big (0.962740)
&lt;span class=&#34;ln&#34;&gt;7&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;튜닝-방법&#34;&gt;튜닝 방법&lt;/h1&gt;
&lt;h3 id=&#34;1-ca의-skip-nodes-with-local-storage-옵션을-false로-설정&#34;&gt;1. CA의 skip-nodes-with-local-storage 옵션을 false로 설정&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/autoscaler/tree/master/charts/cluster-autoscaler&#34;&gt;Helm chart&lt;/a&gt;의 디폴트 값은 true지만 항상 false로 설정하는 것을 권장한다. 경우에 따라 비용 절감 효과가 상당히 클 수도 있다.&lt;/p&gt;
&lt;p&gt;Helm chart를 설치할 때 values.yaml을 아래처럼 설정하면 된다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;extraArgs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;skip-nodes-with-local-storage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;이 옵션이 true이면 emptyDir이나 hostPath를 volume으로 사용하는 pod이 있는 노드를 scale-down 대상에서 제외한다.&lt;/p&gt;
&lt;p&gt;노드가 없어져서 데이터가 유실되는 경우를 막기 true로 설정하는 것이 일견 자연스러워 보이지만 그렇지 않다.
보존해야 하는 데이터라면 애초에 로컬 스토리지를 사용하면 안된다. AWS라면 persistentVolumeClaim을 통해 EBS volume 등에 데이터를 저장해야 한다&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;그런데 이미 모든 app들이 로컬 스토리지에 &amp;lsquo;보존이 필요한 데이터&amp;rsquo;를 저장하지 않는다면 이 옵션을 false로 설정하는 것이 scale-down 효율에 무슨 도움이 될지 의문이 들 수 있다.&lt;/p&gt;
&lt;p&gt;이 옵션 설정이 효과가 있는 이유는 Istio같은 소프트웨어들에서 emptyDir volume을 다른 용도로 사용하기 때문이다.
로컬 스토리지에 데이터를 저장하기 위한 것이 아니라 RAM 기반의 tmpfs 파일시스템을 사용하거나 이것을 매개로 pod 내의 여러 container들이 서로 데이터를 교환하기 위한 목적이다.
이런 것들은 pod이 실행되는 동안에만 의미가 있기 때문에 노드가 없어져도 데이터가 유실되는 문제가 없다.&lt;/p&gt;
&lt;p&gt;경험을 공유하면, scale-down 효율이 떨어지는 클러스터의 CA 로그를 확인해 봤더니 로컬 스토리지 문제 때문에 많은 노드를 scale-down 대상에서 제외한 경우가 있었다. 해당 pod들을 확인하니 Istiod, Istio에 포함된 Prometheus, Redis, SonarQube에 포함된 PostgreSQL이 위에서 설명한 용도로 emptyDir volume을 사용하고 있었다.&lt;/p&gt;
&lt;p&gt;아래는 Istiod pod 설정에서 발췌한 것이다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;istiod-1-10-4-6c66f78678-7wl5r&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;istio-system&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;discovery&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;volumeMounts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;mountPath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/var/run/secrets/istio-dns&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;14&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;local-certs&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;volumes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;17&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;emptyDir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;18&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;medium&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Memory&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;19&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;local-certs&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;emptyDir.medium: Memory&lt;/code&gt; 로 설정하고 있으니 RAM 기반의 tmpfs 파일 시스템을 사용하기 위한 것이다.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-ca의---scale-down-utilization-threshold-옵션-값을-더-크게-설정&#34;&gt;2. CA의 &amp;ndash;scale-down-utilization-threshold 옵션 값을 더 크게 설정.&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/autoscaler/tree/master/charts/cluster-autoscaler&#34;&gt;Helm chart&lt;/a&gt;를 설치할 때 values.yaml을 아래처럼 설정하면 된다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;extraArgs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;scale-down-utilization-threshold&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0.6&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# 디폴트는 0.5. 배포 속도를 고려해 적당한 값으로 설정&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;디폴트 값은 0.5다.
CA가 리소스 활용률이 낮은 노드를 판단하는 기준이다. 노드의 CPU, 메모리 리소스 활용률 중 더 큰 값이 이 설정보다 작으면 scale-down 대상이 된다. 따라서 이 수치를 높이면 대상이 늘어날 수 있다. 하지만 너무 높게 설정하면 pod의 scale-out 이나 배포 속도가 느려질 수 있다. 새로 생성된 pod이 들어갈 공간이 기존 노드에 없어서 새로운 노드를 생성해야 할 확률이 높아지기 때문이다. 실서비스 환경이 아니라면 더 과감하게 올려볼 수 있다.&lt;/p&gt;
&lt;h3 id=&#34;3-poddisruptionbudgetpdb-설정이-scale-down을-막고-있는-상태-해소&#34;&gt;3. PodDisruptionBudget(PDB) 설정이 scale-down을 막고 있는 상태 해소&lt;/h3&gt;
&lt;p&gt;잘못 설정된 PodDisruptionBudget이 scale-down을 막을 수 있다.&lt;/p&gt;
&lt;p&gt;PDB의 개념이 아직 생소하다면 아래 문서를 통해 개념을 정확히 이해할 필요가 있다. 서비스 안정성이나 클러스터 관리 작업에 많은 영향을 주는 중요한 요소이기 때문에 application 개발자와 클러스터 관리자 모두 개념을 정확히 알고 있어야 한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets&#34;&gt;https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/configure-pdb/&#34;&gt;https://kubernetes.io/docs/tasks/run-application/configure-pdb/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;예를 들어, 개발 환경에선 application 당 pod을 1개만 실행하면서 실서비스 환경의 PDB 설정을 그대로 사용하면 문제가 된다.
아래처럼 minAvailable을 30% 로 설정하면 1개만 있는 pod을 종료할 수 없다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;policy/v1beta1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;PodDisruptionBudget&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;demo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;minAvailable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;l&#34;&gt;%&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;matchLabels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;demo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;이런 경우 minAvailable을 0으로 설정하든지 pod 개수를 2개 이상으로 늘려야 한다. 비용이 더 들겠지만 후자가 적절한 방법이라고 생각한다. 개발 환경이 실서비스 환경과 구조적으로 동일해야 개발 환경에서 미리 여러가지 검증을 할 수 있기 때문이다.&lt;/p&gt;
&lt;h3 id=&#34;4-kube-system-namespace-에서-실행되는-pod에-poddisruptionbudget-설정&#34;&gt;4. kube-system namespace 에서 실행되는 pod에 PodDisruptionBudget 설정&lt;/h3&gt;
&lt;p&gt;CA는 kube-system namespace에 속하는 pod이 실행되는 노드를 scale-down 대상에서 제외하지만 PDB로 명시적으로 허용하면 대상에 포함한다.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-to-set-pdbs-to-enable-ca-to-move-kube-system-pods&#34;&gt;https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-to-set-pdbs-to-enable-ca-to-move-kube-system-pods&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;AWS EKS 클러스터라면 아래처럼 CoreDNS에 PDB 설정을 할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl create poddisruptionbudget coredns --namespace=kube-system --selector  eks.amazonaws.com/component=coredns --max-unavailable 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;만약 어쩔 수 없이 로컬 스토리지를 써야하고 노드 종료를 막아야 하는 상황이 있다면 --skip-nodes-with-local-storage 옵션을 true로 설정하는 것 대신 PodDistruptionBudget을 적절히 설정해 주거나
pod의 annotation에 &lt;code&gt;&amp;quot;cluster-autoscaler.kubernetes.io/safe-to-evict&amp;quot;: &amp;quot;false&amp;quot;&lt;/code&gt; 를 붙이는 방법이 있다.하지만 노드가 다른 이유로 셧다운될 때 데이터를 잃어버리는 것은 마찬가지다.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;emptyDir.medium 이 Memory인 경우는 로컬 스토리지를 사용하지 것으로 보지 않는 것이 합리적일 것 같은데
CA는 emptyDir은 모두 로컬 스토리지 사용으로 본다. 이것은 &lt;a href=&#34;https://github.com/kubernetes/autoscaler/issues/2048&#34;&gt;의도된 동작&lt;/a&gt;이라고 한다. --skip-nodes-with-local-storage 옵션이 true일 때 로컬 스토리지를 사용하는 pod을 scale-down 대상에 포함시키려면 pod에 &lt;code&gt;&amp;quot;cluster-autoscaler.kubernetes.io/safe-to-evict&amp;quot;: &amp;quot;true&amp;quot;&lt;/code&gt; 라고 annotation을 붙이면 된다.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
        
      </description>
    </item>
    
    <item>
      <title>오해하기 쉬운 EKS 클러스터의 subnet 설정</title>
      <link>https://pseonghoon.github.io/post/eks-subnet/</link>
      <pubDate>Sat, 17 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pseonghoon.github.io/post/eks-subnet/</guid>
      <description>
        
          &lt;p&gt;EKS 클러스터를 생성할 때 subnet 설정의 의미를 오해하기 쉽다.&lt;/p&gt;
&lt;p&gt;이 설정은 &lt;u&gt;EKS master가 worker node와 통신하기 위해 사용할 subnet&lt;/u&gt;을 지정하는 것이다. 그런데 worker node가 사용하는 모든 subnet을 지정하는 것으로 잘못 이해하면 나중에 worker node들이 사용할 subnet을 추가하기 위해 EKS 클러스터(master)를 재생성해야 하는 것으로 오해할 수 있다.&lt;/p&gt;
&lt;p&gt;특히 Terraform 등 코드로 클러스터를 만들 때 argument의 이름만 보고 오해하기 쉽다.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/cli/latest/reference/eks/create-cluster.html#options&#34;&gt;AWSCLI 문서&lt;/a&gt; 중 클러스터 subnetIds에 대한 설명이 아래와 같다.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;subnetIds -&amp;gt; (list)&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;Specify subnets for your Amazon EKS worker nodes.
Amazon EKS creates cross-account elastic network interfaces in these
subnets to allow communication between your worker nodes and the Kubernetes control plane.&lt;/p&gt;
&lt;p&gt;(string)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;첫 문장을 보고 worker node들이 사용하는 모든 subnet으로 설정해야 한다고 생각할 수 있는데 그렇지 않다. 첫 문장에 오해의 소지가 있다.&lt;/p&gt;
&lt;p&gt;이어지는 내용을 차분히 읽으면 알 수 있듯이, control plane (master)이 해당 subnet에 network interface를 생성해서 worker node들과 통신할 수 있으면 된다.
VPC 내의 subnet들은 서로 라우팅이 되니 &lt;mark&gt; VPC 내의 private subnet 중 아무 것이나 사용해도 된다&lt;/mark&gt;. 단, 서로 다른 AZ에 있는 2개 이상의 subnet으로 설정해야 한다.&lt;/p&gt;
&lt;h2 id=&#34;잘못-이해했을-때의-문제&#34;&gt;잘못 이해했을 때의 문제&lt;/h2&gt;
&lt;p&gt;이것을 잘못 이해하면 곤란한 상황에 처할 수 있다.
EKS에선 Pod들도 실제 VPC의 IP를 사용하니 많은 IP가 필요하다.&lt;/p&gt;
&lt;p&gt;처음에 클러스터의 규모를 작게 예상해서 worker node가 사용하는 subnet에 작은 크기의 IP 대역을 할당할 수 있다.
이후에 IP가 부족하면 subnet을 늘려야 하는데, 설정의 의미를 오해하면 EKS cluster(master)의 subnet도 그에 맞게 변경해야 한다고 생각하게 된다.&lt;/p&gt;
&lt;p&gt;문제는 &lt;a href=&#34;(https://docs.aws.amazon.com/eks/latest/APIReference/API_UpdateClusterConfig.html)&#34;&gt;AWS EKS API&lt;/a&gt;가 subnetIds 속성의 변경을 허용하지 않는다는 것이다.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.aws.amazon.com/eks/latest/APIReference/API_UpdateClusterConfig.html&#34;&gt;API 문서&lt;/a&gt;에서도 아래처럼 알려주고 있다. (2020년 10월 16일 기준)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Important&lt;/strong&gt;
At this time, you can not update the subnets or security group IDs for an existing cluster.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Terraform에서 &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/eks_cluster#subnet_ids&#34;&gt;subnet_ids&lt;/a&gt; 값을 변경하고 terraform apply를 실행하면 클러스터를 삭제하고 새로 생성한다.&lt;/p&gt;
&lt;p&gt;결국 IP 대역을 늘리기 위해 불필요하게 cluster를 삭제하고 새로 만드는 수고를 하게 될 수 있다.
아니면 IP 대역을 늘리는 것이 쉬운 일이 아니라고 생각해서 처음부터 지나치게 큰 IP 대역을 할당할 수도 있다.&lt;/p&gt;
&lt;h2 id=&#34;올바른-설정의-예&#34;&gt;올바른 설정의 예&lt;/h2&gt;
&lt;p&gt;범용으로 사용하는 private subnet (private_subnet이라 하자)과 eks worker 전용 subnet (eks_worker_subnet)을 따로 만들었다고 가정하자.&lt;/p&gt;
&lt;p&gt;EKS 클러스터에는 private_subnet을 설정하면 된다.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Terraform 코드 예제&lt;/strong&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-hcl&#34; data-lang=&#34;hcl&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;resource&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;aws_eks_cluster&amp;#34; &amp;#34;main&amp;#34;&lt;/span&gt; {
&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;  name&lt;/span&gt;     &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;s2&#34;&gt;&amp;#34;main&amp;#34;&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;  role_arn&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;terraform_remote_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;tf_iam&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;common_role_arn&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;eks_master&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;  &lt;span class=&#34;k&#34;&gt;vpc_config&lt;/span&gt; {
&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;    security_group_ids&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;p&#34;&gt;[&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;terraform_remote_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;tf_sg&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;eks_sg_ids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;eks_master&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;]&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;    # 2개 이상의 AZ를 포함하기만 하면 되므로 2개만 잘라서 쓴다. 
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;    # 2개로 고정했기 때문에 나중에 만약 private_subnet_ids에 새로운 subnet이 추가되어도 여기를 수정하지 않아도 된다.
&lt;/span&gt;&lt;span class=&#34;hl&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;&lt;/span&gt;&lt;span class=&#34;n&#34;&gt;    subnet_ids&lt;/span&gt; &lt;span class=&#34;o&#34;&gt;=&lt;/span&gt; &lt;span class=&#34;k&#34;&gt;slice&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;(&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;data&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;terraform_remote_state&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;tf_vpc&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;outputs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;.&lt;/span&gt;&lt;span class=&#34;k&#34;&gt;private_subnet_ids&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;,&lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;)&lt;/span&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;  }
&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;}&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/p&gt;
&lt;p&gt;eks_woker_subnet은 worker node를 생성하는 Auto Scaling Group에 설정하면 된다.
Terraform 이라면 aws_autoscaling_group의 &lt;a href=&#34;https://registry.terraform.io/providers/hashicorp/aws/latest/docs/resources/autoscaling_group#vpc_zone_identifier&#34;&gt;vpc_zone_identifier&lt;/a&gt;에 설정하면 된다.&lt;/p&gt;
&lt;p&gt;이 argument는 Auto Scaling Group 재생성 없이 업데이트가 가능하기 때문에 Pod이 사용할 IP가 모자라면 쉽게 subnet을 추가해서 해결할 수 있다.&lt;/p&gt;
        
      </description>
    </item>
    
    <item>
      <title>Kubernetes에서 NFS 사용하기</title>
      <link>https://pseonghoon.github.io/post/k8s-and-nfs/</link>
      <pubDate>Sun, 11 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pseonghoon.github.io/post/k8s-and-nfs/</guid>
      <description>
        
          &lt;p&gt;Pod에서 NFS를 volume으로 사용하기 위해 &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/volumes/#nfs&#34;&gt;공식 문서&lt;/a&gt;의 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/&#34;&gt;예제&lt;/a&gt;&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;를 참고하는 경우가 많을 것인데, 보완 설명을 하려고 한다. AWS EFS를 NFS 서버로 사용할 때 편리하게 쓸 수 있는 efs-provisioner 도 간략하게 소개한다.&lt;/p&gt;
&lt;p&gt;공식 문서에서 소개하는 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/&#34;&gt;예제&lt;/a&gt;는 클러스터 내부에서 NFS 서버를 운영한다. 그리고 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/nfs-pv.yaml&#34;&gt;PV&lt;/a&gt;(PersistentVolume)와 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/nfs-pvc.yaml&#34;&gt;PVC&lt;/a&gt;(PersistentVolumeClaim)를 정의해서 Pod이 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/nfs-web-rc.yaml#L29-L30&#34;&gt;PVC를 volume으로 지정&lt;/a&gt;하는 방식을 쓰고 있다.&lt;/p&gt;
&lt;p&gt;더 실용적이고 간단한 방법은 PV와 PVC를 만들지 않고 아래처럼 Pod의 volume으로 NFS를 직접 지정하는 것이다. NFS 서버도 클러스터 밖에서 운영하는 것이 좋다. AWS라면 &lt;a href=&#34;https://aws.amazon.com/efs/&#34;&gt;EFS 서비스&lt;/a&gt;를 사용하면 된다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;apps/v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Deployment&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;nfs-example&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;replicas&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; 
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;matchLabels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;nfs-example&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;template&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;labels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;nfs-example&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;14&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;image&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;nginx&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;17&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;nginx&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;18&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;volumeMounts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;19&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;           &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;mountPath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/nfs&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;20&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;             &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;efs-vol&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;hl&#34;&gt;&lt;span class=&#34;ln&#34;&gt;21&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;volumes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;hl&#34;&gt;&lt;span class=&#34;ln&#34;&gt;22&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;efs-vol&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;hl&#34;&gt;&lt;span class=&#34;ln&#34;&gt;23&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;        &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;nfs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;hl&#34;&gt;&lt;span class=&#34;ln&#34;&gt;24&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# 예제로 EFS를 사용했지만 일반적인 NFS 서버면 된다.&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;hl&#34;&gt;&lt;span class=&#34;ln&#34;&gt;25&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;server&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;fs-f9352198.efs.ap-northeast-2.amazonaws.com &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;hl&#34;&gt;&lt;span class=&#34;ln&#34;&gt;26&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;path&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/span&gt;&lt;span class=&#34;hl&#34;&gt;&lt;span class=&#34;ln&#34;&gt;27&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;          &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;readOnly&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;
&lt;p&gt;&lt;em&gt;nfs 타입의 volume에서 설정할 수 있는 field는 &lt;a href=&#34;https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/#nfsvolumesource-v1-core&#34;&gt;API reference&lt;/a&gt;를 참고했다.&lt;/em&gt;&lt;/p&gt;
&lt;h1 id=&#34;official-nfs-example&#34;&gt;공식 문서 예제 보완 설명&lt;/h1&gt;
&lt;p&gt;먼저 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/&#34;&gt;예제&lt;/a&gt;의 내용을 따라하면서 충분히 이해한 후 이 포스트를 읽으면 더 도움이 될 것이다.
예제는 관련 개념을 파악하는데 도움이 되지만 실무에서 활용하려면 유의할 점이 있다.&lt;/p&gt;
&lt;p&gt;우선 NFS 서버를 클러스터 내부에서 운영하고 있다. 예제로는 좋은 접근이지만 그대로 활용하기에는 아래와 같은 한계가 있다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;NFS 서버의 위치를 DNS name 대신 IP로 하드코딩한다.&lt;/li&gt;
&lt;li&gt;Google Cloud의 Persistent Disk를 사용했다. Persistent Disk는 zone에 종속된다. (AWS라면 EBS와 AZ에 해당)&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;그리고 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/nfs-pv.yaml&#34;&gt;PV&lt;/a&gt;와 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/nfs-pvc.yaml&#34;&gt;PVC&lt;/a&gt;를 정의한 후 Pod이 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/nfs-web-rc.yaml#L29-L30&#34;&gt;PVC를 volume으로 지정&lt;/a&gt;하는데, 맥락 상 어색한 방법이다.&lt;/p&gt;
&lt;h4 id=&#34;hardcodedip&#34;&gt;IP 하드코딩&lt;/h4&gt;
&lt;p&gt;예제에선 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/nfs-server-service.yaml&#34;&gt;nfs-server&lt;/a&gt; 라는 Service를 정의한다. 그래서 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/nfs-pv.yaml&#34;&gt;PV 정의&lt;/a&gt;에서 DNS name인 &lt;strong&gt;nfs-server.default.svc.cluster.local&lt;/strong&gt; 로 NFS 서버의 위치를 지정하고 있지만 이렇게는 접속이 안된다. 해결하기 위한 임시방편으로 Service의 clusterIP를 사람이 확인해서 코드를 고치는 방식을 안내하고 있다.&lt;/p&gt;
&lt;p&gt;여기서 DNS name을 사용할 수 없는 이유가 있다. NFS mount는 Pod이 직접 하는 것이 아니라 node에서 해줘야 한다. 그런데 node는 클러스터 내의 DNS(coreDNS 등)에 쿼리를 하지 않기 때문이다. 그렇게 하도록 설정하는 것이 가능하겠지만 node의 DNS lookup이 Kubernetes 클러스터 내부 DNS를 의존하는 것은 좋은 구성이 아닐 수 있다.&lt;/p&gt;
&lt;h4 id=&#34;pvc&#34;&gt;PVC와 PV의 어색한 활용&lt;/h4&gt;
&lt;p&gt;Pod의 생애와 관계 없이 계속 유지되는 데이터가 필요하면 PV(PersistentVolume)를 반드시 써야 한다고 생각하기 쉽다. 하지만 꼭 그런 것은 아니다.&lt;/p&gt;
&lt;p&gt;PV는 PVC(PersistentVolumeClaim)없이 단독으로 volume으로 사용할 수 없다.
그럼 PVC는 왜 필요한가? PVC는 대부분의 경우 dynamic provisioning 하기 위해 사용한다. 클라우드 환경이라면 더욱 그렇다.&lt;/p&gt;
&lt;p&gt;예제에서는 이미 존재하는 NFS 서버와 path(/)를 mount한 것이기 때문에 PVC와 PV를 굳이 정의할 필요가 없다.
이 포스트에서 한 것처럼 Pod의 volume에서 바로 nfs를 사용하는 것이 더 간단하고 직관적이다.&lt;/p&gt;
&lt;p&gt;그리고, 예제에서 PVC와 PV를 binding한 방식도 적절하지 않다.
이 주제에서 별로 중요한 포인트는 아니라서 각주&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;에서 설명했으니 관심 있는 분들은 참고하셨으면 한다.&lt;/p&gt;
&lt;h1 id=&#34;efs-provisioner-소개&#34;&gt;efs-provisioner 소개&lt;/h1&gt;
&lt;p&gt;AWS에서 EFS를 사용한다면, &lt;a href=&#34;https://github.com/kubernetes-retired/external-storage/tree/master/aws/efs&#34;&gt;efs-provisioner&lt;/a&gt;를 쓰면 편리하다.&lt;/p&gt;
&lt;p&gt;오해하기 쉬운데, EFS를 Kubernets에서 사용하기 위해서 반드시 efs-provisioner를 써야하는 것은 아니다. efs-provisioner는 EFS에 기반한 volume을 dynamic provisioning할 때 사용하는 것이다. 이 포스트에서 주로 다룬 경우처럼 이미 NFS 서버와 mount할 path가 존재한다면 적합하지 않은 도구이다.&lt;/p&gt;
&lt;p&gt;이것을 사용하려면 PVC를 정의해야 하는데 PVC는 namespaced resource라서 동일한 namespace 안의 Pod만 접근할 수 있는 한계가 있다.&lt;/p&gt;
&lt;h4 id=&#34;사용-방법&#34;&gt;사용 방법&lt;/h4&gt;
&lt;p&gt;PVC를 여럿 만드는 경우에도, EFS file system을 하나만 생성하고 efs-provisioner &lt;a href=&#34;https://github.com/helm/charts/tree/master/stable/efs-provisioner&#34;&gt;Helm chart&lt;/a&gt;도 하나만 설치하면 된다.&lt;/p&gt;
&lt;p&gt;Helm chart를 설치할 때 EFS filesystem ID를 알려주면, 이것과 연계된 StorageClass를 생성한다.
여러 Pod들이 공유해야 하는 데이터가 있으면 이 StorageClass를 사용하는 PVC를 생성한 후 여러 Pod들이 동일하게 이 PVC를 volume으로 지정하면 된다.
그러면 efs-provisioner가 EFS filesystem 내에 각 PVC별로 전용 디렉토리를 만들어 제공하는 방식이다.&lt;/p&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;참조하는 코드는 2020년 10월 10일 기준 master branch가 가리키는 commit으로 고정했다.&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;공식문서의 예제에선 PVC에 이미 존재하는 PV를 binding 했다. 이것을 위해 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/nfs-pvc.yaml#L8&#34;&gt;PVC의 storageClassName 설정&lt;/a&gt;을 “”로 했는데 &lt;a href=&#34;https://github.com/kubernetes/examples/blob/bbe33f4997d781cffe9e310281e5ab4da07a07d5/staging/volumes/nfs/nfs-pv.yaml&#34;&gt;PV&lt;/a&gt;는 storageClassName을 정의하지 않았기 때문에 binding이 된 것 같다. 이런 일을 정확히 하려면 PV에 label을 달고 PVC에 &lt;a href=&#34;https://kubernetes.io/docs/concepts/storage/persistent-volumes/#selector&#34;&gt;label selector&lt;/a&gt;를 설정해야 할 것이다.&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
        
      </description>
    </item>
    
  </channel>
</rss>
