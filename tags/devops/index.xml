<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>DevOps on 박성훈의 블로그</title>
    <link>https://pseonghoon.github.io/tags/devops/</link>
    <description>Recent content in DevOps on 박성훈의 블로그</description>
    <generator>Hugo -- gohugo.io</generator>
    <copyright>2020</copyright>
    <lastBuildDate>Wed, 24 Nov 2021 00:00:00 +0000</lastBuildDate><atom:link href="https://pseonghoon.github.io/tags/devops/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>[Kubernetes] Cluster Autocaler의 scale-down 튜닝</title>
      <link>https://pseonghoon.github.io/post/ca-scale-down-optimization/</link>
      <pubDate>Wed, 24 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pseonghoon.github.io/post/ca-scale-down-optimization/</guid>
      <description>
        
          &lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler&#34;&gt;Cluster Autoscaler(CA)&lt;/a&gt;는 클러스터의 노드 개수를 적절하게 유지하는 역할을 한다.
새로 생성된 pod이 실행될 노드가 없어 &lt;code&gt;Pending&lt;/code&gt; 상태라면 노드를 생성하고,
반대로 필요한 리소스에 비해 노드 수가 불필요하게 많다면 한가한 노드를 없애서 리소스 집적도를 높이는 scale-down을 한다.&lt;/p&gt;
&lt;p&gt;CA가 특정 노드를 없애려면 몇가지 제약 조건에 해당하지 않아야 하는데, 이런 제약 조건 때문에 scale-down 효율이 떨어지면 불필요한 비용을 지출하게 될 수 있다.&lt;/p&gt;
&lt;p&gt;CA의 scale-down 효율을 높여서 비용 절감에 효과를 본 방법을 정리했다. 요약하면 아래와 같다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;--skip-nodes-with-local-storage 옵션을 false로 설정&lt;/li&gt;
&lt;li&gt;--scale-down-utilization-threshold 옵션값 올리기&lt;/li&gt;
&lt;li&gt;application의 PodDisruptionBudget(PDB)가 scale-down을 막고 있는 경우 PDB 수정&lt;/li&gt;
&lt;li&gt;kube-system namespace에 속한 pod에 PodDisruptionBudget을 설정 (kube-dns 등)&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;cluster-autoscaler의-scale-down-조건&#34;&gt;Cluster Autoscaler의 scale-down 조건&lt;/h1&gt;
&lt;p&gt;Clusster Autosacler &lt;a href=&#34;https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#what-types-of-pods-can-prevent-ca-from-removing-a-node&#34;&gt;FAQ 문서&lt;/a&gt;에 잘 기술되어 있다.&lt;/p&gt;
&lt;p&gt;노드에 아래 조건에 해당하는 pod이 있으면 CA가 노드를 제거할 수 없다. 이 포스트의 맥락에 해당하는 것만 추렸다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;로컬 스토리지를 사용하는 경우&lt;/li&gt;
&lt;li&gt;PodDisruptionBudget에 의해 pod을 종료할 수 없는 경우&lt;/li&gt;
&lt;li&gt;kube-system namespace에 속한 pod. 적절한 PodDisruptionBudget 설정을 해주면 이 제한을 풀 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;현재-노드-개수가-적절한지-확인하는-방법&#34;&gt;현재 노드 개수가 적절한지 확인하는 방법&lt;/h1&gt;
&lt;p&gt;우선 현재 scale-down을 충분히 효율적으로 하고 있는지 확인해야 한다.
노드의 &amp;lsquo;리소스 활용률&amp;rsquo; 이 충분히 높아야 하는데, 아래처럼 계산할 수 있다.&lt;/p&gt;
&lt;p&gt;&lt;code&gt;리소스 활용률 = (노드에서 실행되는 컨테이너들의 resource request 합)/(노드의 resource capacity)&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;컨테이너에서 실행되는 프로세스들이 실제 사용하는 수치가 아니라 컨테이너의 resource 설정에서 cpu나 memory request 값을 따지는 것이다.&lt;/p&gt;
&lt;p&gt;노드별 리스스 활용률은 kubectl로 대략 파악할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl describe nodes | grep Resource --after=3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;kubectl에 &lt;a href=&#34;https://github.com/etopeter/kubectl-view-utilization&#34;&gt;view-utilization plugin&lt;/a&gt;을 설치하면 더 쉽게 확인할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl view-utilization &lt;span class=&#34;c1&#34;&gt;# 클러스터 전체 현황&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;$ kubectl view-utilization nodes &lt;span class=&#34;c1&#34;&gt;# 노드별 현황&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;아래는 kubectl view-utilization 명령의 실행 결과 예시이다.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://pseonghoon.github.io/post/img/kubectl_view_utilization.png&#34;/&gt; 
&lt;/figure&gt;

&lt;p&gt;제대로 관리하려면 결국 Prometheus 같은 모니터링 시스템의 관리 대상에 포함해야 한다.&lt;/p&gt;
&lt;p&gt;kube-state-metrics가 제공하는 metric을 조회하는 Prometheus 쿼리는 아래와 같다.&lt;/p&gt;
&lt;p&gt;클러스터 CPU 리소스 활용률 평균: &lt;code&gt;sum (kube_pod_container_resource_requests{resource=&amp;quot;cpu&amp;quot;}) / sum (kube_node_status_capacity{resource=&amp;quot;cpu&amp;quot;} )&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;클러스터 메모리 리소스 활용률 평균: &lt;code&gt;sum (kube_pod_container_resource_requests{resource=&amp;quot;memory&amp;quot;}) / sum (kube_node_status_capacity{resource=&amp;quot;memory&amp;quot;} )&lt;/code&gt;&lt;/p&gt;
&lt;h1 id=&#34;ca의-로그로-scale-down을-하지-않는-이유-확인&#34;&gt;CA의 로그로 scale-down을 하지 않는 이유 확인&lt;/h1&gt;
&lt;p&gt;CA는 특정 노드를 scale-down 대상으로 취급하지 않는지 이유를 로그로 자세히 알려준다.
아래 명령으로 확인할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl logs -f  -n kube-system &lt;span class=&#34;k&#34;&gt;$(&lt;/span&gt;kubectl get pod -o Name -n kube-system -l app.kubernetes.io/instance&lt;span class=&#34;o&#34;&gt;=&lt;/span&gt;cluster-autoscaler&lt;span class=&#34;k&#34;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;로그의 예는 아래와 같다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;I1119 16:13:22.567346       1 cluster.go:148] Fast evaluation: ip-10-76-141-178.ap-northeast-2.compute.internal for removal
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;I1119 16:13:22.567357       1 cluster.go:169] Fast evaluation: node ip-10-76-141-178.ap-northeast-2.compute.internal cannot be removed: pod with local storage present: istiod-1-10-4-6c66f78678-7wl5r
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;...
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;5&lt;/span&gt;I1119 16:18:57.113554       1 scale_down.go:443] Node ip-10-91-71-213.ap-northeast-2.compute.internal is not suitable for removal - cpu utilization too big (0.917098)
&lt;span class=&#34;ln&#34;&gt;6&lt;/span&gt;I1119 16:18:57.113607       1 scale_down.go:443] Node ip-10-91-70-122.ap-northeast-2.compute.internal is not suitable for removal - memory utilization too big (0.962740)
&lt;span class=&#34;ln&#34;&gt;7&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;튜닝-방법&#34;&gt;튜닝 방법&lt;/h1&gt;
&lt;h3 id=&#34;1-ca의-skip-nodes-with-local-storage-옵션을-false로-설정&#34;&gt;1. CA의 skip-nodes-with-local-storage 옵션을 false로 설정&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/autoscaler/tree/master/charts/cluster-autoscaler&#34;&gt;Helm chart&lt;/a&gt;의 디폴트 값은 true지만 항상 false로 설정하는 것을 권장한다. 경우에 따라 비용 절감 효과가 상당히 클 수도 있다.&lt;/p&gt;
&lt;p&gt;Helm chart를 설치할 때 values.yaml을 아래처럼 설정하면 된다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;extraArgs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;skip-nodes-with-local-storage&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;kc&#34;&gt;false&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;이 옵션이 true이면 emptyDir이나 hostPath를 volume으로 사용하는 pod이 있는 노드를 scale-down 대상에서 제외한다.&lt;/p&gt;
&lt;p&gt;노드가 없어져서 데이터가 유실되는 경우를 막기 true로 설정하는 것이 일견 자연스러워 보이지만 그렇지 않다.
보존해야 하는 데이터라면 애초에 로컬 스토리지를 사용하면 안된다. AWS라면 persistentVolumeClaim을 통해 EBS volume 등에 데이터를 저장해야 한다&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;그런데 이미 모든 app들이 로컬 스토리지에 &amp;lsquo;보존이 필요한 데이터&amp;rsquo;를 저장하지 않는다면 이 옵션을 false로 설정하는 것이 scale-down 효율에 무슨 도움이 될지 의문이 들 수 있다.&lt;/p&gt;
&lt;p&gt;이 옵션 설정이 효과가 있는 이유는 Istio같은 소프트웨어들에서 emptyDir volume을 다른 용도로 사용하기 때문이다.
로컬 스토리지에 데이터를 저장하기 위한 것이 아니라 RAM 기반의 tmpfs 파일시스템을 사용하거나 이것을 매개로 pod 내의 여러 container들이 서로 데이터를 교환하기 위한 목적이다.
이런 것들은 pod이 실행되는 동안에만 의미가 있기 때문에 노드가 없어져도 데이터가 유실되는 문제가 없다.&lt;/p&gt;
&lt;p&gt;경험을 공유하면, scale-down 효율이 떨어지는 클러스터의 CA 로그를 확인해 봤더니 로컬 스토리지 문제 때문에 많은 노드를 scale-down 대상에서 제외한 경우가 있었다. 해당 pod들을 확인하니 Istiod, Istio에 포함된 Prometheus, Redis, SonarQube에 포함된 PostgreSQL이 위에서 설명한 용도로 emptyDir volume을 사용하고 있었다.&lt;/p&gt;
&lt;p&gt;아래는 Istiod pod 설정에서 발췌한 것이다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;ln&#34;&gt; 1&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;v1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Pod&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 4&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 5&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;istiod-1-10-4-6c66f78678-7wl5r&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 6&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;namespace&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;istio-system&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 7&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 8&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt; 9&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;containers&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;10&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;discovery&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;11&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;12&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;volumeMounts&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;13&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;mountPath&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;/var/run/secrets/istio-dns&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;14&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;local-certs&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;15&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;...&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;16&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;volumes&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;17&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;emptyDir&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;18&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;medium&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;Memory&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;19&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;local-certs&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;emptyDir.medium: Memory&lt;/code&gt; 로 설정하고 있으니 RAM 기반의 tmpfs 파일 시스템을 사용하기 위한 것이다.&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-ca의---scale-down-utilization-threshold-옵션-값을-더-크게-설정&#34;&gt;2. CA의 &amp;ndash;scale-down-utilization-threshold 옵션 값을 더 크게 설정.&lt;/h3&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/autoscaler/tree/master/charts/cluster-autoscaler&#34;&gt;Helm chart&lt;/a&gt;를 설치할 때 values.yaml을 아래처럼 설정하면 된다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;extraArgs&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;scale-down-utilization-threshold&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;0.6&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;c&#34;&gt;# 디폴트는 0.5. 배포 속도를 고려해 적당한 값으로 설정&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;디폴트 값은 0.5다.
CA가 리소스 활용률이 낮은 노드를 판단하는 기준이다. 노드의 CPU, 메모리 리소스 활용률 중 더 큰 값이 이 설정보다 작으면 scale-down 대상이 된다. 따라서 이 수치를 높이면 대상이 늘어날 수 있다. 하지만 너무 높게 설정하면 pod의 scale-out 이나 배포 속도가 느려질 수 있다. 새로 생성된 pod이 들어갈 공간이 기존 노드에 없어서 새로운 노드를 생성해야 할 확률이 높아지기 때문이다. 실서비스 환경이 아니라면 더 과감하게 올려볼 수 있다.&lt;/p&gt;
&lt;h3 id=&#34;3-poddisruptionbudgetpdb-설정이-scale-down을-막고-있는-경우-해소&#34;&gt;3. PodDisruptionBudget(PDB) 설정이 scale-down을 막고 있는 경우 해소&lt;/h3&gt;
&lt;p&gt;잘못 설정된 PodDisruptionBudget이 scale-down을 막을 수 있다.&lt;/p&gt;
&lt;p&gt;PDB의 개념이 아직 생소하다면 아래 문서를 통해 개념을 정확히 이해할 필요가 있다. 서비스 안정성이나 클러스터 관리 작업에 많은 영향을 주는 중요한 요소이기 때문에 application 개발자와 클러스터 관리자 모두 개념을 정확히 알고 있어야 한다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets&#34;&gt;https://kubernetes.io/docs/concepts/workloads/pods/disruptions/#pod-disruption-budgets&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://kubernetes.io/docs/tasks/run-application/configure-pdb/&#34;&gt;https://kubernetes.io/docs/tasks/run-application/configure-pdb/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;예를 들어, 개발 환경에선 application 당 pod을 1개만 실행하면서 실서비스 환경의 PDB 설정을 그대로 사용하면 문제가 된다.
아래처럼 minAvailable을 30% 로 설정하면 1개만 있는 pod을 종료할 수 없다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;apiVersion&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;policy/v1beta1&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;kind&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;PodDisruptionBudget&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;metadata&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;demo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;spec&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;minAvailable&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;m&#34;&gt;30&lt;/span&gt;&lt;span class=&#34;l&#34;&gt;%&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;selector&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;    &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;matchLabels&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;9&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;      &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;app&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;demo&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;이런 경우 minAvailable을 0으로 설정하든지 pod 개수를 2개 이상으로 늘려야 한다. 비용이 더 들겠지만 후자가 적절한 방법이라고 생각한다. 개발 환경이 실서비스 환경과 구조적으로 동일해야 개발 환경에서 미리 여러가지 검증을 할 수 있기 때문이다.&lt;/p&gt;
&lt;h3 id=&#34;4-kube-system-namespace-에서-실행되는-pod에-poddisruptionbudget-설정&#34;&gt;4. kube-system namespace 에서 실행되는 pod에 PodDisruptionBudget 설정&lt;/h3&gt;
&lt;p&gt;CA는 kube-system namespace에 속하는 pod이 실행되는 노드를 scale-down 대상에서 제외하지만 PDB로 명시적으로 허용하면 대상에 포함한다.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-to-set-pdbs-to-enable-ca-to-move-kube-system-pods&#34;&gt;https://github.com/kubernetes/autoscaler/blob/master/cluster-autoscaler/FAQ.md#how-to-set-pdbs-to-enable-ca-to-move-kube-system-pods&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;AWS EKS 클러스터라면 아래처럼 CoreDNS에 PDB 설정을 할 수 있다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ kubectl create poddisruptionbudget coredns --namespace=kube-system --selector  eks.amazonaws.com/component=coredns --max-unavailable 1
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;만약 어쩔 수 없이 로컬 스토리지를 써야하고 노드 종료를 막아야 하는 상황이 있다면 --skip-nodes-with-local-storage 옵션을 true로 설정하는 것 대신 PodDistruptionBudget을 적절히 설정해 주거나
pod의 annotation에 &lt;code&gt;&amp;quot;cluster-autoscaler.kubernetes.io/safe-to-evict&amp;quot;: &amp;quot;false&amp;quot;&lt;/code&gt; 를 붙이는 방법이 있다.하지만 노드가 다른 이유로 셧다운될 때 데이터를 잃어버리는 것은 마찬가지다. &lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;emptyDir.medium 이 Memory인 경우는 로컬 스토리지를 사용하지 것으로 보지 않는 것이 합리적일 것 같은데
CA는 emptyDir은 모두 로컬 스토리지 사용으로 본다. 이것은 &lt;a href=&#34;https://github.com/kubernetes/autoscaler/issues/2048&#34;&gt;의도된 동작&lt;/a&gt;이라고 한다. --skip-nodes-with-local-storage 옵션이 true일 때 로컬 스토리지를 사용하는 pod을 scale-down 대상에 포함시키려면 pod에 &lt;code&gt;&amp;quot;cluster-autoscaler.kubernetes.io/safe-to-evict&amp;quot;: &amp;quot;true&amp;quot;&lt;/code&gt; 라고 annotation을 붙이면 된다. &lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
        
      </description>
    </item>
    
    <item>
      <title>로컬에서 개발 중인 Ansible role을 편하게 테스트하는 방법</title>
      <link>https://pseonghoon.github.io/post/testing-ansible-role/</link>
      <pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate>
      
      <guid>https://pseonghoon.github.io/post/testing-ansible-role/</guid>
      <description>
        
          &lt;p&gt;개발 중인 Ansible role을 편리하게 테스트하기 위해 사용하고 있는 방법을 공유한다. Ansible이 제시하는 정석대로 role을 다루는 경우에 적합한 방법이라 그 정석에 대해서도 살펴본다.&lt;/p&gt;
&lt;p&gt;이어질 내용을 요약하면 아래와 같다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Ansible role을 다루는 정석
&lt;ol&gt;
&lt;li&gt;role 하나 당 Git repository 하나&lt;/li&gt;
&lt;li&gt;ansible-galaxy로 필요한 role들을 다운로드&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;symbolic link를 쓰면 개발 중인 role의 테스트를 편하게 할 수 있다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;ansible-role&#34;&gt;Ansible Role&lt;/h1&gt;
&lt;p&gt;Ansible playbook을 여러개 만들다 보면 중복되는 코드가 생긴다. 동일한 코드가 여러 벌이 되면 관리가 불가능해지기 때문에 이런 것들은 &amp;lsquo;모듈&amp;rsquo; 혹은 &amp;lsquo;라이브러리&amp;rsquo; 로 만들어 재사용해야 하는데 Ansible에선 role이 그런 역할을 한다.&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html&#34;&gt;https://docs.ansible.com/ansible/latest/user_guide/playbooks_reuse_roles.html&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;ansible-role을-다루는-정석&#34;&gt;Ansible role을 다루는 정석&lt;/h1&gt;
&lt;h3 id=&#34;1-role-당-git-repository-하나&#34;&gt;1. role 당 Git repository 하나&lt;/h3&gt;
&lt;p&gt;모든 코드는 Git으로 관리해야 한다. Ansible role도 예외가 아니다.&lt;/p&gt;
&lt;p&gt;Ansible role의 Git repository는 어떤 단위로 만들어야 할까? 하나의 Git repository 에서 여러 role들을 포함하는 방법을 생각할 수도 있는데 role마다 Git repository를 만드는 것이 Ansible이 제시하는 체계에 맞는 방법이다.&lt;/p&gt;
&lt;p&gt;전자를 생각했다면 role 하나의 코드 분량이 많지 않은데 전용 Git repository를 만드는 것은 비효율적이라는 생각 때문일 것이다. &lt;u&gt;하지만 Git repository는 관리 대상 코드가 논리적으로 독립된 것이기만 하면 코드의 분량과 관계 없이 만들 수 있고 그렇게 하는 것이 바람직하다&lt;/u&gt;. Git repository 하나가 방대한 리눅스 커널 코드를 관리할 수도 있지만 스크립트 하나를 관리해도 문제될 것이 없다. 관리 비용 때문에 repository 수를 쉽게 늘리기 어려운 Subversion과 차별화되는 점이다.&lt;/p&gt;
&lt;p&gt;개별 Git repository로 관리하면 아래와 같은 장점이 있다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ansible-galaxy가 role을 다운로드하는 체계에 부합한다. requirements.yml에서 필요한 role마다 Git repository와 branch를 설정하는 방식이다.&lt;/li&gt;
&lt;li&gt;각각의 role 별로 원하는 branch의 코드를 사용할 수 있다. master branch에 merge되기 전 개발용 branch의 코드를 사용할 수 있다.&lt;/li&gt;
&lt;li&gt;로컬에서 role을 수정하면서 테스트하기 편리하다.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;2-ansible-galaxy로-다운로드&#34;&gt;2. ansible-galaxy로 다운로드&lt;/h3&gt;
&lt;p&gt;playbook이 있는 디렉토리에 &lt;code&gt;requirements.yml&lt;/code&gt; 을 만들고 다운로드할 role의 목록을 작성한다.
예를 들어 아래처럼 한다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-yaml&#34; data-lang=&#34;yaml&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://git.psh.kr/ansible_role/ec2_linux_common.git&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;scm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;git&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;name&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;ec2_linux_common&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;version&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;master &lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;6&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;&lt;/span&gt;- &lt;span class=&#34;nt&#34;&gt;src&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;https://git.psh.kr/ansible_role/cloudwatch_agent.git&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;7&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;scm&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;git&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;span class=&#34;ln&#34;&gt;8&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;  &lt;/span&gt;&lt;span class=&#34;nt&#34;&gt;version&lt;/span&gt;&lt;span class=&#34;p&#34;&gt;:&lt;/span&gt;&lt;span class=&#34;w&#34;&gt; &lt;/span&gt;&lt;span class=&#34;l&#34;&gt;master&lt;/span&gt;&lt;span class=&#34;w&#34;&gt;
&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;아래처럼 필요한 role들을 다운로드하는 스크립트를 만들어 쓰면 편리하다. 파일 이름은 &lt;code&gt;get_roles.sh&lt;/code&gt; 이라고 하자.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;ansible-galaxy install --force --ignore-errors -r requirements.yml -p roles
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;이 스크립트는 &lt;code&gt;requirements.yml&lt;/code&gt;에 기술된 role들을 roles/ 디렉토리에 다운로드한다.
Ansible playbook은 기본적으로 roles/ 디렉토리에서 role을 찾는다.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;--force 옵션은 이미 다운받은 role을 최신 코드로 업데이트하기 위해 필요하다.&lt;/li&gt;
&lt;li&gt;--ignore-errors 옵션은 뒤에 설명할 symbolic link를 이용해 테스트하는 방법을 쓸 때 ansible-galaxy가 중간에 에러를 내고 멈추지 않게 한다.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;아래는 &lt;code&gt;roles/.gitignore&lt;/code&gt; 의 내용이다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-fallback&#34; data-lang=&#34;fallback&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;*
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;!.gitignore
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;.gitignore&lt;/code&gt;를 제외한 모든 다운받은 role 코드를 Git이 무시하게 한다.&lt;/p&gt;
&lt;h1 id=&#34;symbolic-link로-테스트&#34;&gt;Symbolic link로 테스트&lt;/h1&gt;
&lt;p&gt;위에서 설명한 것처럼 &lt;code&gt;requirements.yml&lt;/code&gt;에 필요한 role을 기술해서 다운로드 받아 써야 한다. 하지만 로컬에서 수정 중인 role을 테스트할 때도 같은 방식을 쓰면 코드 변경을 할때마다 git commit, push 후 ansible-galaxy 실행을 반복해야 해서 불편하다.&lt;/p&gt;
&lt;p&gt;해결책은 roles/ 아래에 다운로드한 디렉토리를 삭제하고, 작업 중인 로컬 Git repository를 가리키는 symbolic link를 만드는 것이다. ansible-galaxy 명령을 --force 옵션으로 실행해도 symbolic link는 덮어쓰지 않는다는 점을 활용한 것이다.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre class=&#34;chroma&#34;&gt;&lt;code class=&#34;language-bash&#34; data-lang=&#34;bash&#34;&gt;&lt;span class=&#34;ln&#34;&gt;1&lt;/span&gt;$ &lt;span class=&#34;nb&#34;&gt;cd&lt;/span&gt; roles 
&lt;span class=&#34;ln&#34;&gt;2&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# 다운로드한 role 삭제&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;3&lt;/span&gt;$ rm -rf ec2_linux_common
&lt;span class=&#34;ln&#34;&gt;4&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;5&lt;/span&gt;&lt;span class=&#34;c1&#34;&gt;# symbolic link인 ec2_linux_common 이 작업 디렉토리인  ~/git/ansible_role/ec2_linux_common 을 가리키게 된다.&lt;/span&gt;
&lt;span class=&#34;ln&#34;&gt;6&lt;/span&gt;$ ln -s ~/git/ansible_role/ec2_linux_common .  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;이렇게 하면 코드를 수정한 후 commit, push할 필요 없이 role을 사용하는 playbook을 바로 실행해 테스트할 수 있다.&lt;/p&gt;
&lt;p&gt;아래는 &lt;code&gt;get_roles.sh&lt;/code&gt; 을 실행한 결과이다. ansible-galaxy가 warning 메시지를 출력하지만 symbolic link는 보존된다.&lt;/p&gt;
&lt;figure&gt;
    &lt;img src=&#34;https://pseonghoon.github.io/post/img/get_roles_result.png&#34;/&gt; 
&lt;/figure&gt;
        
      </description>
    </item>
    
    <item>
      <title>코드로 인프라 관리하기 (IaC)</title>
      <link>https://pseonghoon.github.io/post/infrastructure-as-code/</link>
      <pubDate>Mon, 05 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>https://pseonghoon.github.io/post/infrastructure-as-code/</guid>
      <description>
        
          &lt;p&gt;&lt;code&gt;링크&lt;/code&gt;: &lt;a href=&#34;https://drive.google.com/file/d/1Bsc5hic_p8nNfGTSa8Dz4UXx49PVFOd4/view?usp=sharing&#34;&gt;코드로 인프라 관리하기 (ppt)&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Games on AWS 2019에서 발표한 자료다. 당시 청중 설문에서 가장 좋은 평가를 받았다.
Infrastructure as Code, 더 나아가 자동화를 통해 무엇을 얻을 수 있는지, 잘 정착되게 하려면 어떤 것을 신경써야 하는지를 정리했다.&lt;/p&gt;
&lt;p&gt;사내에서 경영진 대상으로 발표했던 자료를 행사의 성격에 맞게 수정해서 사용했다.
2017년부터 2년 간의 구현 경험으로 쓴 것이다.&lt;/p&gt;
        
      </description>
    </item>
    
  </channel>
</rss>
